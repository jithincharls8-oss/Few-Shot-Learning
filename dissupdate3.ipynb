{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running minimal unit check...\n",
      "OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1754757338.076268 4204849 gl_context.cc:369] GL version: 2.1 (2.1 INTEL-22.5.11), renderer: Intel(R) Iris(TM) Plus Graphics OpenGL Engine (1x6x8 (fused) LP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1754757338.229874 4207319 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1754757338.384625 4207319 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "# Deterministic, self calibrating few-shot gesture recognizer.\n",
    "# DCT per channel over time, PCA whitening to embed_dim, Gaussian head with shrinkage,\n",
    "# optional conformal abstention, SafetyGate for hold to confirm.\n",
    "# MediaPipe is optional, the code falls back gracefully.\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import io\n",
    "import json\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple, Optional, Dict, Any, Deque\n",
    "from collections import deque, defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Optional MediaPipe\n",
    "try:\n",
    "    import mediapipe as mp\n",
    "    _HAS_MP = True\n",
    "except Exception:\n",
    "    mp = None\n",
    "    _HAS_MP = False\n",
    "\n",
    "# -----------------------------\n",
    "# Configs\n",
    "# -----------------------------\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    # sequence config\n",
    "    seq_len: int = 32\n",
    "    min_len: int = 12\n",
    "    frame_dim: int = 42            # 21 points, x y\n",
    "    use_deltas: bool = True\n",
    "\n",
    "    # deterministic encoder\n",
    "    dct_k_pos: int = 12            # keep lowest K temporal freqs per channel\n",
    "    dct_k_delta: int = 10\n",
    "    pca_embed_dim: int = 128       # embedding size after PCA\n",
    "    pca_whiten: bool = True\n",
    "    energy_norm: bool = False\n",
    "\n",
    "    # safety gate\n",
    "    conf_thresh: float = 0.70\n",
    "    smooth_k: int = 7\n",
    "    ema_alpha: float = 0.9\n",
    "    confirm_secs: float = 2.5\n",
    "\n",
    "    # Gaussian head and uncertainty\n",
    "    logit_scale: float = 5.0\n",
    "    cov_reg: float = 1e-2\n",
    "    shrinkage_lambda: float = 0.20\n",
    "    shrink_target: str = \"identity\"\n",
    "\n",
    "    # Conformal abstention\n",
    "    conformal_alpha: float = 0.10\n",
    "    conformal_mondrian: bool = True\n",
    "    calib_frac: float = 0.25\n",
    "\n",
    "    # augmentation\n",
    "    pos_noise_std: float = 0.01\n",
    "    time_warp_prob: float = 0.0\n",
    "    time_warp_segments: int = 3\n",
    "    time_warp_sigma: float = 0.20\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class GateConfig:\n",
    "    conf_thresh: float\n",
    "    smooth_k: int\n",
    "    ema_alpha: float\n",
    "    confirm_secs: float\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Gesture:\n",
    "    name: str\n",
    "    action: str\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Safety gate, continuous hold\n",
    "# -----------------------------\n",
    "\n",
    "class SafetyGate:\n",
    "    def __init__(self, cfg: GateConfig) -> None:\n",
    "        self.cfg = cfg\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        self.ema_conf = 0.0\n",
    "        self.hist: Deque[int] = deque(maxlen=self.cfg.smooth_k)\n",
    "        self.state = \"idle\"\n",
    "        self.pending_idx: Optional[int] = None\n",
    "        self.hold_s: float = 0.0\n",
    "        self._last_t: Optional[float] = None\n",
    "\n",
    "    def update(self, idx: int, conf: float, now: float, conf_thresh_override: Optional[float] = None) -> str:\n",
    "        self._last_t = self._last_t or now\n",
    "        dt = max(0.0, now - self._last_t)\n",
    "        self._last_t = now\n",
    "\n",
    "        self.ema_conf = self.cfg.ema_alpha * self.ema_conf + (1 - self.cfg.ema_alpha) * max(conf, 0.0)\n",
    "        thresh = float(conf_thresh_override) if conf_thresh_override is not None else self.cfg.conf_thresh\n",
    "\n",
    "        if idx >= 0 and self.ema_conf >= thresh:\n",
    "            self.hist.append(idx)\n",
    "            items = list(self.hist)\n",
    "            if not items:\n",
    "                self.state = \"arming\"\n",
    "                return self.state\n",
    "            majority = max(set(items), key=items.count)\n",
    "            need = max(2, math.ceil(self.cfg.smooth_k * 0.6))\n",
    "            stable = items.count(majority) >= need\n",
    "            if stable:\n",
    "                if self.pending_idx != majority:\n",
    "                    self.pending_idx = majority\n",
    "                    self.hold_s = 0.0\n",
    "                self.hold_s += dt\n",
    "                self.state = \"confirm\" if self.hold_s >= self.cfg.confirm_secs else \"countdown\"\n",
    "            else:\n",
    "                self.state = \"arming\"\n",
    "        else:\n",
    "            self.state = \"arming\"\n",
    "            self.hold_s = max(0.0, self.hold_s - 0.5 * dt)\n",
    "        return self.state\n",
    "\n",
    "    def remaining(self, now: float) -> float:\n",
    "        return max(0.0, self.cfg.confirm_secs - self.hold_s)\n",
    "\n",
    "    def decide_fire(self, cancel: bool, now: float) -> bool:\n",
    "        if cancel:\n",
    "            self.reset()\n",
    "            return False\n",
    "        ok = self.state == \"confirm\"\n",
    "        self.reset()\n",
    "        return ok\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Async camera\n",
    "# -----------------------------\n",
    "\n",
    "import threading\n",
    "import cv2\n",
    "\n",
    "class VideoCaptureAsync:\n",
    "    def __init__(self, src: int = 0, width: Optional[int] = None, height: Optional[int] = None):\n",
    "        self.src = src\n",
    "        self.cap = cv2.VideoCapture(src)\n",
    "        if not self.cap.isOpened():\n",
    "            raise RuntimeError(f\"Could not open camera index {src}\")\n",
    "        # lower latency if backend supports it\n",
    "        try:\n",
    "            self.cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)\n",
    "        except Exception:\n",
    "            pass\n",
    "        if width is not None:\n",
    "            self.cap.set(cv2.CAP_PROP_FRAME_WIDTH, width)\n",
    "        if height is not None:\n",
    "            self.cap.set(cv2.CAP_PROP_FRAME_HEIGHT, height)\n",
    "        self.q: Deque[np.ndarray] = deque(maxlen=1)\n",
    "        self.lock = threading.Lock()\n",
    "        self.stop_flag = threading.Event()\n",
    "        self.th = threading.Thread(target=self._worker, daemon=True)\n",
    "        self.th.start()\n",
    "\n",
    "    def _worker(self) -> None:\n",
    "        miss = 0\n",
    "        while not self.stop_flag.is_set():\n",
    "            ok, frame = self.cap.read()\n",
    "            if not ok or frame is None:\n",
    "                miss += 1\n",
    "                if miss > 50:\n",
    "                    try:\n",
    "                        self.cap.release()\n",
    "                        time.sleep(0.1)\n",
    "                        self.cap = cv2.VideoCapture(self.src)\n",
    "                        try:\n",
    "                            self.cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)\n",
    "                        except Exception:\n",
    "                            pass\n",
    "                    except Exception:\n",
    "                        pass\n",
    "                    miss = 0\n",
    "                time.sleep(0.02)\n",
    "                continue\n",
    "            miss = 0\n",
    "            with self.lock:\n",
    "                self.q.clear()\n",
    "                self.q.append(frame)\n",
    "\n",
    "    def read(self) -> Tuple[bool, Optional[np.ndarray]]:\n",
    "        with self.lock:\n",
    "            if self.q:\n",
    "                return True, self.q[-1].copy()\n",
    "        ok, frame = self.cap.read()\n",
    "        return ok, frame if ok else None\n",
    "\n",
    "    def stop(self) -> None:\n",
    "        self.stop_flag.set()\n",
    "\n",
    "    def release(self) -> None:\n",
    "        try:\n",
    "            self.stop()\n",
    "        except Exception:\n",
    "            pass\n",
    "        try:\n",
    "            if self.th.is_alive():\n",
    "                self.th.join(timeout=0.5)\n",
    "        except Exception:\n",
    "            pass\n",
    "        try:\n",
    "            self.cap.release()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "\n",
    "class LandmarkSmoother:\n",
    "    def __init__(self, window_size: int = 5):\n",
    "        self.k = max(1, int(window_size))\n",
    "        self.buf: Deque[np.ndarray] = deque(maxlen=self.k)\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        self.buf.clear()\n",
    "\n",
    "    def apply(self, pts_xy: np.ndarray) -> np.ndarray:\n",
    "        self.buf.append(pts_xy)\n",
    "        arr = np.stack(list(self.buf), axis=0)\n",
    "        return arr.mean(axis=0)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Orientation normalizer\n",
    "# -----------------------------\n",
    "\n",
    "class OrientationNormalizer:\n",
    "    \"\"\"\n",
    "    Smooth scale and rotation per stream. Reset when you clear the window or switch modes.\n",
    "    \"\"\"\n",
    "    def __init__(self, max_deg_step: float = 10.0, scale_alpha: float = 0.9):\n",
    "        self.max_step = math.radians(max_deg_step)\n",
    "        self.scale_alpha = float(scale_alpha)\n",
    "        self.prev_theta: Optional[float] = None\n",
    "        self.prev_scale: Optional[float] = None\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        self.prev_theta = None\n",
    "        self.prev_scale = None\n",
    "\n",
    "    def normalize(self, rel: np.ndarray) -> np.ndarray:\n",
    "        v = rel[9] - rel[0]\n",
    "        ang = math.atan2(v[1], v[0])\n",
    "        theta = (math.pi / 2.0) - ang\n",
    "        if self.prev_theta is None:\n",
    "            self.prev_theta = float(theta)\n",
    "        else:\n",
    "            dtheta = float(theta - self.prev_theta)\n",
    "            dtheta = max(-self.max_step, min(self.max_step, dtheta))\n",
    "            self.prev_theta = self.prev_theta + dtheta\n",
    "        c = math.cos(self.prev_theta)\n",
    "        s = math.sin(self.prev_theta)\n",
    "        R = np.array([[c, -s], [s, c]], dtype=np.float32)\n",
    "        return rel @ R.T\n",
    "\n",
    "    def smooth_scale(self, scale: float) -> float:\n",
    "        if self.prev_scale is None:\n",
    "            self.prev_scale = float(scale)\n",
    "        else:\n",
    "            self.prev_scale = self.scale_alpha * self.prev_scale + (1.0 - self.scale_alpha) * float(scale)\n",
    "        return max(1e-6, float(self.prev_scale))\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Landmarks, rotation normalization\n",
    "# -----------------------------\n",
    "\n",
    "def _init_mp_hands():\n",
    "    hands = mp.solutions.hands.Hands(\n",
    "        static_image_mode=False,\n",
    "        max_num_hands=2,\n",
    "        min_detection_confidence=0.5,\n",
    "        min_tracking_confidence=0.5,\n",
    "        model_complexity=1,\n",
    "    )\n",
    "    return hands\n",
    "\n",
    "_MP_CTX = _init_mp_hands() if _HAS_MP else None\n",
    "\n",
    "def extract_landmarks(\n",
    "    frame_bgr: np.ndarray,\n",
    "    smoother: Optional[LandmarkSmoother] = None,\n",
    "    orient: Optional[OrientationNormalizer] = None,\n",
    "    canonicalize_left_to_right: bool = True\n",
    ") -> Tuple[Optional[torch.Tensor], Optional[str]]:\n",
    "    if not _HAS_MP or _MP_CTX is None:\n",
    "        return None, None\n",
    "    h, w = frame_bgr.shape[:2]\n",
    "    img_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n",
    "    res = _MP_CTX.process(img_rgb)\n",
    "    if not res.multi_hand_landmarks:\n",
    "        return None, None\n",
    "\n",
    "    idx_best = 0\n",
    "    handed = None\n",
    "    if res.multi_handedness:\n",
    "        scores = []\n",
    "        for i, hd in enumerate(res.multi_handedness):\n",
    "            try:\n",
    "                scores.append((i, hd.classification[0].score, hd.classification[0].label))\n",
    "            except Exception:\n",
    "                scores.append((i, 0.0, None))\n",
    "        scores.sort(key=lambda t: t[1], reverse=True)\n",
    "        idx_best, _, handed = scores[0]\n",
    "    lms = res.multi_hand_landmarks[idx_best]\n",
    "\n",
    "    pts = np.array([[lm.x, lm.y] for lm in lms.landmark], dtype=np.float32)\n",
    "    if smoother is not None:\n",
    "        pts_px = pts.copy()\n",
    "        pts_px[:, 0] *= w\n",
    "        pts_px[:, 1] *= h\n",
    "        pts_px = smoother.apply(pts_px)\n",
    "        pts = pts_px.copy()\n",
    "        pts[:, 0] /= max(1, w)\n",
    "        pts[:, 1] /= max(1, h)\n",
    "\n",
    "    center = pts[0:1, :]\n",
    "    rel = pts - center\n",
    "\n",
    "    scale = np.linalg.norm(pts[9] - pts[0]) + 1e-6\n",
    "    if not np.isfinite(scale) or scale < 1e-3:\n",
    "        min_xy = pts.min(axis=0)\n",
    "        max_xy = pts.max(axis=0)\n",
    "        scale = float(np.linalg.norm(max_xy - min_xy) + 1e-6)\n",
    "    if orient is not None:\n",
    "        scale = orient.smooth_scale(float(scale))\n",
    "    rel = rel / max(1e-6, float(scale))\n",
    "\n",
    "    if canonicalize_left_to_right and handed and handed.lower().startswith(\"left\"):\n",
    "        rel[:, 0] = -rel[:, 0]\n",
    "\n",
    "    if orient is not None:\n",
    "        rel = orient.normalize(rel)\n",
    "\n",
    "    return torch.from_numpy(rel.astype(np.float32)), handed\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Feature building and aug\n",
    "# -----------------------------\n",
    "\n",
    "def build_feature(lm_xy_rel: torch.Tensor, prev_lm_xy_rel: Optional[torch.Tensor], use_deltas: bool) -> Optional[torch.Tensor]:\n",
    "    if lm_xy_rel is None or not torch.is_tensor(lm_xy_rel):\n",
    "        return None\n",
    "    cur = lm_xy_rel.reshape(-1)\n",
    "    if use_deltas:\n",
    "        if prev_lm_xy_rel is not None and prev_lm_xy_rel.numel() == lm_xy_rel.numel():\n",
    "            delta = (lm_xy_rel - prev_lm_xy_rel).reshape(-1)\n",
    "        else:\n",
    "            delta = torch.zeros_like(cur)\n",
    "        feat = torch.cat([cur, delta], dim=0)\n",
    "    else:\n",
    "        feat = cur\n",
    "    if torch.any(torch.isnan(feat)):\n",
    "        return None\n",
    "    return feat.float()\n",
    "\n",
    "\n",
    "def resample_sequence(seq_td: torch.Tensor, target_len: int) -> torch.Tensor:\n",
    "    assert seq_td.dim() == 2\n",
    "    T, D = seq_td.shape\n",
    "    if T == target_len:\n",
    "        return seq_td\n",
    "    src_idx = torch.linspace(0, T - 1, steps=target_len)\n",
    "    idx0 = torch.clamp(src_idx.floor().long(), 0, T - 1)\n",
    "    idx1 = torch.clamp(idx0 + 1, 0, T - 1)\n",
    "    w = (src_idx - idx0.float()).unsqueeze(1)\n",
    "    out = (1 - w) * seq_td[idx0] + w * seq_td[idx1]\n",
    "    return out\n",
    "\n",
    "\n",
    "def time_warp(seq_td: torch.Tensor, segments: int = 3, sigma: float = 0.2) -> torch.Tensor:\n",
    "    T, D = seq_td.shape\n",
    "    if T < 3 or segments < 1 or sigma <= 0:\n",
    "        return seq_td\n",
    "    knots = sorted(np.random.choice(np.arange(1, T - 1), size=min(segments, max(1, T // 4)), replace=False).tolist())\n",
    "    points = [0] + knots + [T - 1]\n",
    "    slopes = np.clip(np.random.normal(loc=1.0, scale=sigma, size=len(points) - 1), a_min=0.3, a_max=2.5)\n",
    "    deltas = np.array([points[i + 1] - points[i] for i in range(len(points) - 1)], dtype=np.float32)\n",
    "    warped_deltas = deltas * slopes\n",
    "    t_warp = np.concatenate([[0.0], np.cumsum(warped_deltas)])\n",
    "    t_warp *= (T - 1) / max(t_warp[-1], 1e-6)\n",
    "    grid = np.linspace(0, T - 1, num=T, dtype=np.float32)\n",
    "\n",
    "    out = torch.empty_like(seq_td)\n",
    "    seq_np = seq_td.detach().cpu().numpy()\n",
    "    for d in range(D):\n",
    "        vals = np.interp(grid, t_warp, seq_np[points, d])\n",
    "        out[:, d] = torch.from_numpy(vals).to(dtype=seq_td.dtype)\n",
    "    return out.to(device=seq_td.device)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Secure store, PBKDF2 with salt\n",
    "# -----------------------------\n",
    "\n",
    "class SecureStore:\n",
    "    \"\"\"\n",
    "    Writes artifacts.pt under the project dir.\n",
    "    If GESTURE_STORE_PASSPHRASE is set and cryptography is available, encrypts with PBKDF2 derived key and a per file salt.\n",
    "    Set GESTURE_STORE_REQUIRE_ENCRYPTION=1 to refuse plaintext.\n",
    "    \"\"\"\n",
    "    def __init__(self, project_dir: str):\n",
    "        self.project_dir = project_dir\n",
    "        os.makedirs(self.project_dir, exist_ok=True)\n",
    "        self.path = os.path.join(self.project_dir, \"artifacts.pt\")\n",
    "        self._fernet_cls = None\n",
    "        self._pass = \"\"\n",
    "        self._require = False\n",
    "        self._init_crypto()\n",
    "\n",
    "    def _init_crypto(self) -> None:\n",
    "        self._require = os.environ.get(\"GESTURE_STORE_REQUIRE_ENCRYPTION\", \"0\") == \"1\"\n",
    "        self._pass = os.environ.get(\"GESTURE_STORE_PASSPHRASE\", \"\") or \"\"\n",
    "        try:\n",
    "            from cryptography.fernet import Fernet  # type: ignore\n",
    "            self._fernet_cls = Fernet\n",
    "        except Exception:\n",
    "            self._fernet_cls = None\n",
    "\n",
    "    def _derive_key(self, salt: bytes) -> Optional[bytes]:\n",
    "        if not self._pass or self._fernet_cls is None:\n",
    "            return None\n",
    "        import base64, hashlib\n",
    "        key = hashlib.pbkdf2_hmac(\"sha256\", self._pass.encode(\"utf-8\"), salt, 200_000, dklen=32)\n",
    "        return base64.urlsafe_b64encode(key)\n",
    "\n",
    "    def save_artifacts(self, payload: Dict[str, Any]) -> None:\n",
    "        try:\n",
    "            raw = io.BytesIO()\n",
    "            torch.save(payload, raw)\n",
    "            data = raw.getvalue()\n",
    "            if self._fernet_cls is not None and self._pass:\n",
    "                salt = os.urandom(16)\n",
    "                fkey = self._derive_key(salt)\n",
    "                if fkey is None:\n",
    "                    raise RuntimeError(\"KDF failed\")\n",
    "                f = self._fernet_cls(fkey)\n",
    "                data = b\"ENC1\" + salt + f.encrypt(data)\n",
    "            elif self._require:\n",
    "                raise RuntimeError(\"Encryption required but unavailable\")\n",
    "            with open(self.path, \"wb\") as f:\n",
    "                f.write(data)\n",
    "        except Exception as e:\n",
    "            print(f\"[SecureStore] save failed: {e}\")\n",
    "\n",
    "    def load_artifacts(self) -> Optional[Dict[str, Any]]:\n",
    "        if not os.path.exists(self.path):\n",
    "            return None\n",
    "        try:\n",
    "            data = open(self.path, \"rb\").read()\n",
    "            if data[:4] == b\"ENC1\":\n",
    "                salt = data[4:20]\n",
    "                ct = data[20:]\n",
    "                fkey = self._derive_key(salt)\n",
    "                if fkey is None or self._fernet_cls is None:\n",
    "                    print(\"[SecureStore] cannot decrypt, missing passphrase or cryptography\")\n",
    "                    return None\n",
    "                f = self._fernet_cls(fkey)\n",
    "                try:\n",
    "                    data = f.decrypt(ct)\n",
    "                except Exception:\n",
    "                    print(\"[SecureStore] decrypt failed, wrong passphrase\")\n",
    "                    return None\n",
    "            elif self._require:\n",
    "                print(\"[SecureStore] plaintext refused by policy\")\n",
    "                return None\n",
    "            buf = io.BytesIO(data)\n",
    "            payload = torch.load(buf, map_location=\"cpu\")\n",
    "            return payload\n",
    "        except Exception as e:\n",
    "            print(f\"[SecureStore] load failed: {e}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Deterministic encoder: DCT + PCA\n",
    "# -----------------------------\n",
    "\n",
    "class DeterministicEncoder(nn.Module):\n",
    "    def __init__(self, cfg: ModelConfig, device: torch.device):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.device = device\n",
    "        self.T = cfg.seq_len\n",
    "        self.pos_dim = cfg.frame_dim\n",
    "        self.delta_dim = cfg.frame_dim if cfg.use_deltas else 0\n",
    "        self._C_pos = self._make_dct_basis(self.T, cfg.dct_k_pos).to(device)\n",
    "        self._C_del = self._make_dct_basis(self.T, cfg.dct_k_delta).to(device) if self.delta_dim else None\n",
    "\n",
    "        # PCA parameters will be learned from support\n",
    "        self.register_buffer(\"pca_mean\", torch.zeros(1))\n",
    "        self.register_buffer(\"pca_components\", torch.zeros(1))  # E x F\n",
    "        self.register_buffer(\"pca_scale\", torch.ones(1))        # 1 x E\n",
    "        self._pca_ready = False\n",
    "\n",
    "    @staticmethod\n",
    "    def _make_dct_basis(T: int, K: int) -> torch.Tensor:\n",
    "        n = torch.arange(T).float().unsqueeze(1)  # T x 1\n",
    "        k = torch.arange(K).float().unsqueeze(0)  # 1 x K\n",
    "        basis = torch.cos(math.pi / T * (n + 0.5) * k)  # T x K\n",
    "        basis[:, 0] *= math.sqrt(0.5)\n",
    "        basis = basis * math.sqrt(2.0 / T)\n",
    "        return basis  # T x K\n",
    "\n",
    "    def _split_pos_delta(self, seq_bt_d: torch.Tensor) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n",
    "        if self.delta_dim:\n",
    "            pos = seq_bt_d[:, :, :self.pos_dim]\n",
    "            delt = seq_bt_d[:, :, self.pos_dim:self.pos_dim + self.delta_dim]\n",
    "            return pos, delt\n",
    "        else:\n",
    "            return seq_bt_d, None\n",
    "\n",
    "    def _dct_features(self, x_btd: torch.Tensor, C: torch.Tensor, k_keep: int) -> torch.Tensor:\n",
    "        B, T, Dch = x_btd.shape\n",
    "        x2 = x_btd.reshape(T, B * Dch)  # T x (B*D)\n",
    "        proj = torch.matmul(C[:, :k_keep].T, x2)  # K x (B*D)\n",
    "        proj = proj.reshape(k_keep, B, Dch).permute(1, 0, 2)  # B x K x D\n",
    "        if self.cfg.energy_norm:\n",
    "            eps = 1e-8\n",
    "            norm = torch.linalg.norm(proj, dim=1, keepdims=True).clamp_min(eps)\n",
    "            proj = proj / norm\n",
    "        return proj.reshape(B, -1)\n",
    "\n",
    "    def seq_to_basefeat(self, seq_bt_d: torch.Tensor) -> torch.Tensor:\n",
    "        pos, delt = self._split_pos_delta(seq_bt_d)\n",
    "        f_pos = self._dct_features(pos, self._C_pos, self.cfg.dct_k_pos)\n",
    "        if delt is not None and self._C_del is not None:\n",
    "            f_del = self._dct_features(delt, self._C_del, self.cfg.dct_k_delta)\n",
    "            base = torch.cat([f_pos, f_del], dim=1)\n",
    "        else:\n",
    "            base = f_pos\n",
    "        return base\n",
    "\n",
    "    def fit_pca(self, feats_bf: torch.Tensor) -> None:\n",
    "        mean = feats_bf.mean(dim=0, keepdim=True)\n",
    "        X = feats_bf - mean\n",
    "        E = min(self.cfg.pca_embed_dim, min(X.shape[0], X.shape[1]))\n",
    "        # try low rank PCA when beneficial\n",
    "        try:\n",
    "            U, S, V = torch.pca_lowrank(X, q=E, center=False)\n",
    "            comps = V[:, :E].T\n",
    "            S_use = S[:E]\n",
    "        except Exception:\n",
    "            U, S, Vh = torch.linalg.svd(X, full_matrices=False)\n",
    "            comps = Vh[:E, :]\n",
    "            S_use = S[:E]\n",
    "        if self.cfg.pca_whiten:\n",
    "            n = max(1, X.shape[0] - 1)\n",
    "            scale = (math.sqrt(n) / S_use.clamp_min(1e-8)).unsqueeze(0)\n",
    "        else:\n",
    "            scale = torch.ones(1, E, device=comps.device)\n",
    "        self.pca_mean = mean.detach()\n",
    "        self.pca_components = comps.detach()\n",
    "        self.pca_scale = scale.detach()\n",
    "        self._pca_ready = True\n",
    "\n",
    "    def transform(self, feats_bf: torch.Tensor) -> torch.Tensor:\n",
    "        if not self._pca_ready:\n",
    "            E = min(self.cfg.pca_embed_dim, feats_bf.shape[1])\n",
    "            z = F.normalize(feats_bf[:, :E], p=2, dim=1)\n",
    "            return z\n",
    "        X = feats_bf - self.pca_mean\n",
    "        Z = torch.matmul(X, self.pca_components.t())\n",
    "        Z = Z * self.pca_scale\n",
    "        Z = F.normalize(Z, p=2, dim=1)\n",
    "        return Z\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def encode(self, seq_bt_d: torch.Tensor) -> torch.Tensor:\n",
    "        base = self.seq_to_basefeat(seq_bt_d)\n",
    "        z = self.transform(base)\n",
    "        return z\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Gaussian head with shrinkage and conformal\n",
    "# -----------------------------\n",
    "\n",
    "class GaussianHead:\n",
    "    def __init__(self, embed_dim: int, cfg: ModelConfig):\n",
    "        self.embed_dim = embed_dim\n",
    "        self.cfg = cfg\n",
    "        self.mu: Optional[torch.Tensor] = None             # K x E\n",
    "        self.cov: Optional[torch.Tensor] = None            # K x E x E\n",
    "        self.chol: Optional[torch.Tensor] = None           # K x E x E\n",
    "        self.logdet: Optional[torch.Tensor] = None         # K\n",
    "        self.K: int = 0\n",
    "        self.q_global: Optional[float] = None\n",
    "        self.q_per_class: Dict[int, float] = {}\n",
    "\n",
    "    @staticmethod\n",
    "    def _shrink_target(sample_cov: torch.Tensor, mode: str) -> torch.Tensor:\n",
    "        E = sample_cov.shape[-1]\n",
    "        if mode == \"identity\":\n",
    "            trace = torch.trace(sample_cov)\n",
    "            target = (trace / E) * torch.eye(E, dtype=sample_cov.dtype, device=sample_cov.device)\n",
    "        elif mode == \"diag\":\n",
    "            target = torch.diag(torch.diag(sample_cov))\n",
    "        else:\n",
    "            trace = torch.trace(sample_cov)\n",
    "            target = (trace / E) * torch.eye(E, dtype=sample_cov.dtype, device=sample_cov.device)\n",
    "        return target\n",
    "\n",
    "    def fit(self, embs: torch.Tensor, labels: torch.Tensor) -> None:\n",
    "        device = embs.device\n",
    "        labels = labels.to(torch.long)\n",
    "        classes = labels.unique(sorted=True)\n",
    "        K = classes.numel()\n",
    "        E = embs.shape[1]\n",
    "        mu_list, cov_list, chol_list, logdet_list = [], [], [], []\n",
    "        lam = float(self.cfg.shrinkage_lambda)\n",
    "        base_reg = float(self.cfg.cov_reg)\n",
    "\n",
    "        for k in classes.tolist():\n",
    "            idx = labels == k\n",
    "            z = embs[idx]\n",
    "            n_k = z.shape[0]\n",
    "            if n_k < 4:\n",
    "                mu_k = z.mean(dim=0) if n_k > 0 else torch.zeros(E, device=device)\n",
    "                var = z.var(dim=0, unbiased=True) if n_k > 1 else torch.ones(E, device=device)\n",
    "                cov_k = torch.diag(var + base_reg)\n",
    "            else:\n",
    "                mu_k = z.mean(dim=0)\n",
    "                D = (z - mu_k)\n",
    "                S = (D.t() @ D) / max(1, n_k - 1)\n",
    "                S = S + base_reg * torch.eye(E, device=device)\n",
    "                Tgt = self._shrink_target(S, self.cfg.shrink_target)\n",
    "                cov_k = (1.0 - lam) * S + lam * Tgt\n",
    "            reg = 0.0\n",
    "            L = None\n",
    "            for _ in range(5):\n",
    "                L_try, info = torch.linalg.cholesky_ex(cov_k + reg * torch.eye(E, device=device))\n",
    "                if int(info.item()) == 0:\n",
    "                    L = L_try\n",
    "                    break\n",
    "                reg = max(1e-6, 10.0 * (reg if reg > 0 else base_reg))\n",
    "            if L is None:\n",
    "                L = torch.linalg.cholesky(cov_k + (base_reg + 1e-3) * torch.eye(E, device=device))\n",
    "            logdet_k = 2.0 * torch.log(torch.diag(L)).sum()\n",
    "            mu_list.append(mu_k)\n",
    "            cov_list.append(cov_k)\n",
    "            chol_list.append(L)\n",
    "            logdet_list.append(logdet_k)\n",
    "        self.mu = torch.stack(mu_list, dim=0)\n",
    "        self.cov = torch.stack(cov_list, dim=0)\n",
    "        self.chol = torch.stack(chol_list, dim=0)\n",
    "        self.logdet = torch.stack(logdet_list, dim=0)\n",
    "        self.K = K\n",
    "\n",
    "    def _m2_batch(self, z: torch.Tensor) -> torch.Tensor:\n",
    "        assert self.mu is not None and self.chol is not None\n",
    "        diff = z.unsqueeze(0) - self.mu  # K x E\n",
    "        b = diff.unsqueeze(-1)                       # K x E x 1\n",
    "        y = torch.cholesky_solve(b, self.chol)       # K x E x 1\n",
    "        m2 = (diff * y.squeeze(-1)).sum(dim=1)       # K\n",
    "        return m2\n",
    "\n",
    "    def nll(self, z: torch.Tensor) -> torch.Tensor:\n",
    "        assert self.logdet is not None\n",
    "        m2 = self._m2_batch(z)\n",
    "        E = z.shape[-1]\n",
    "        nll = 0.5 * (m2 + self.logdet + E * math.log(2 * math.pi))\n",
    "        return nll\n",
    "\n",
    "    def predict_with_margin(self, z: torch.Tensor) -> Tuple[int, float, float, float]:\n",
    "        nll = self.nll(z)\n",
    "        logits = -self.cfg.logit_scale * nll\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        conf, idx = probs.max(dim=-1)\n",
    "        if probs.numel() >= 2:\n",
    "            top2 = torch.topk(probs, k=2)\n",
    "            margin = float(top2.values[0].item() - top2.values[1].item())\n",
    "        else:\n",
    "            margin = float(conf.item())\n",
    "        return int(idx.item()), float(conf.item()), float(nll[idx].item()), margin\n",
    "\n",
    "    @staticmethod\n",
    "    def _quantile(scores: List[float], alpha: float) -> float:\n",
    "        n = len(scores)\n",
    "        if n == 0:\n",
    "            return float(\"inf\")\n",
    "        k = int(math.ceil((n + 1) * (1 - alpha)))\n",
    "        k = min(max(1, k), n)\n",
    "        s = sorted(scores)\n",
    "        return s[k - 1]\n",
    "\n",
    "    def fit_conformal(self, embs: torch.Tensor, labels: torch.Tensor) -> None:\n",
    "        assert self.mu is not None\n",
    "        scores_by_class: Dict[int, List[float]] = defaultdict(list)\n",
    "        all_scores: List[float] = []\n",
    "        for z, y in zip(embs, labels.long()):\n",
    "            nll = self.nll(z)\n",
    "            s = float(nll[y].item())\n",
    "            all_scores.append(s)\n",
    "            scores_by_class[int(y.item())].append(s)\n",
    "        if self.cfg.conformal_mondrian:\n",
    "            self.q_per_class = {}\n",
    "            for c, arr in scores_by_class.items():\n",
    "                self.q_per_class[c] = self._quantile(arr, self.cfg.conformal_alpha) if len(arr) >= 3 else float(\"inf\")\n",
    "            self.q_global = None\n",
    "        else:\n",
    "            self.q_global = self._quantile(all_scores, self.cfg.conformal_alpha)\n",
    "            self.q_per_class = {}\n",
    "\n",
    "    def abstain(self, pred_idx: int, nonconformity: float) -> bool:\n",
    "        if self.cfg.conformal_mondrian and self.q_per_class:\n",
    "            q = self.q_per_class.get(pred_idx, float(\"inf\"))\n",
    "            return nonconformity > q\n",
    "        if self.q_global is not None:\n",
    "            return nonconformity > self.q_global\n",
    "        return False\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Model manager\n",
    "# -----------------------------\n",
    "\n",
    "class ModelManager:\n",
    "    def __init__(self, cfg: ModelConfig, device: torch.device):\n",
    "        self.cfg = cfg\n",
    "        d = cfg.frame_dim * (2 if cfg.use_deltas else 1)\n",
    "        self.frame_dim = d\n",
    "        self.device = device\n",
    "        self.encoder = DeterministicEncoder(cfg, device=self.device).to(self.device)\n",
    "        self.embed_dim = cfg.pca_embed_dim\n",
    "        self.gestures: List[Gesture] = []\n",
    "        self.seqs: List[torch.Tensor] = []\n",
    "        self.labels: List[int] = []\n",
    "        self.gauss = GaussianHead(embed_dim=self.embed_dim, cfg=cfg)\n",
    "\n",
    "    def _find_gesture_id(self, name: str) -> Optional[int]:\n",
    "        for i, g in enumerate(self.gestures):\n",
    "            if g.name == name:\n",
    "                return i\n",
    "        return None\n",
    "\n",
    "    def ensure_gesture(self, name: str, action: str) -> int:\n",
    "        gid = self._find_gesture_id(name)\n",
    "        if gid is not None:\n",
    "            if self.gestures[gid].action != action:\n",
    "                self.gestures[gid] = Gesture(name=name, action=action)\n",
    "            return gid\n",
    "        gid = len(self.gestures)\n",
    "        self.gestures.append(Gesture(name=name, action=action))\n",
    "        return gid\n",
    "\n",
    "    def add_sample(self, name_or_id: Any, seq: torch.Tensor) -> int:\n",
    "        if isinstance(name_or_id, int):\n",
    "            gid = int(name_or_id)\n",
    "            if gid < 0 or gid >= len(self.gestures):\n",
    "                raise IndexError(\"gesture id out of range\")\n",
    "        else:\n",
    "            gid = self.ensure_gesture(str(name_or_id), action=self._default_action(str(name_or_id)))\n",
    "        if seq.dim() == 3 and seq.shape[0] == 1:\n",
    "            seq = seq.squeeze(0)\n",
    "        if seq.dim() != 2:\n",
    "            raise ValueError(\"seq must be (T, D) or (1, T, D)\")\n",
    "        s = resample_sequence(seq.detach().cpu().float(), self.cfg.seq_len)\n",
    "        for aug in self._augment_sequence(s):\n",
    "            self.seqs.append(aug)\n",
    "            self.labels.append(gid)\n",
    "        self.update_geometry_and_conformal()\n",
    "        return gid\n",
    "\n",
    "    def add_example(self, name: str, seq: torch.Tensor) -> int:\n",
    "        return self.add_sample(name, seq)\n",
    "\n",
    "    def _default_action(self, name: str) -> str:\n",
    "        return name\n",
    "\n",
    "    def remove_gesture(self, idx: int) -> None:\n",
    "        if idx < 0 or idx >= len(self.gestures):\n",
    "            raise IndexError(\"gesture id out of range\")\n",
    "        del self.gestures[idx]\n",
    "        new_seqs, new_labels = [], []\n",
    "        for s, y in zip(self.seqs, self.labels):\n",
    "            if y == idx:\n",
    "                continue\n",
    "            new_seqs.append(s)\n",
    "            new_labels.append(y - 1 if y > idx else y)\n",
    "        self.seqs, self.labels = new_seqs, new_labels\n",
    "        self.update_geometry_and_conformal()\n",
    "\n",
    "    @property\n",
    "    def num_classes(self) -> int:\n",
    "        return len(self.gestures)\n",
    "\n",
    "    def load_encoder_weights(self, path: str) -> None:\n",
    "        raise RuntimeError(\"Deterministic encoder does not use learned weights\")\n",
    "\n",
    "    def apply_temperature(self, T: float) -> None:\n",
    "        pass\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def encode(self, seq_bt_d: torch.Tensor) -> torch.Tensor:\n",
    "        self.encoder.eval()\n",
    "        return self.encoder.encode(seq_bt_d)\n",
    "\n",
    "    def _augment_sequence(self, s: torch.Tensor) -> List[torch.Tensor]:\n",
    "        out: List[torch.Tensor] = []\n",
    "        T, D = s.shape\n",
    "        pos_dim = min(self.cfg.frame_dim, D)\n",
    "        pos = s[:, :pos_dim].clone()\n",
    "        has_delta = self.cfg.use_deltas and D >= 2 * self.cfg.frame_dim\n",
    "        if has_delta:\n",
    "            out.append(s)\n",
    "        else:\n",
    "            out.append(pos)\n",
    "        if self.cfg.pos_noise_std > 0:\n",
    "            noisy_pos = pos + torch.randn_like(pos) * self.cfg.pos_noise_std\n",
    "            if self.cfg.use_deltas:\n",
    "                d = torch.zeros_like(noisy_pos)\n",
    "                d[1:] = noisy_pos[1:] - noisy_pos[:-1]\n",
    "                noisy = torch.cat([noisy_pos, d], dim=1)\n",
    "            else:\n",
    "                noisy = noisy_pos\n",
    "            out.append(noisy)\n",
    "        if random.random() < self.cfg.time_warp_prob:\n",
    "            warped_pos = time_warp(pos, segments=self.cfg.time_warp_segments, sigma=self.cfg.time_warp_sigma)\n",
    "            if self.cfg.use_deltas:\n",
    "                d = torch.zeros_like(warped_pos)\n",
    "                d[1:] = warped_pos[1:] - warped_pos[:-1]\n",
    "                warped = torch.cat([warped_pos, d], dim=1)\n",
    "            else:\n",
    "                warped = warped_pos\n",
    "            out.append(warped)\n",
    "        return out\n",
    "\n",
    "    def add_gesture_support(self, name: str, action: str, support: List[torch.Tensor]) -> None:\n",
    "        assert len(support) > 0\n",
    "        gid = len(self.gestures)\n",
    "        self.gestures.append(Gesture(name=name, action=action))\n",
    "        for s in support:\n",
    "            s = resample_sequence(s.detach().cpu().float(), self.cfg.seq_len)\n",
    "            for aug in self._augment_sequence(s):\n",
    "                self.seqs.append(aug)\n",
    "                self.labels.append(gid)\n",
    "        self.update_geometry_and_conformal()\n",
    "\n",
    "    def mirror_sequence(self, seq_td: torch.Tensor) -> torch.Tensor:\n",
    "        T, D = seq_td.shape\n",
    "        out = seq_td.clone()\n",
    "        pos_dim = self.cfg.frame_dim if D >= self.cfg.frame_dim else D\n",
    "        xs = out[:, 0:pos_dim:2]\n",
    "        out[:, 0:pos_dim:2] = -xs\n",
    "        if D > pos_dim:\n",
    "            dxs = out[:, pos_dim:D:2]\n",
    "            out[:, pos_dim:D:2] = -dxs\n",
    "        return out\n",
    "\n",
    "    def _compute_embeddings_and_fit_pca(self) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        if len(self.seqs) == 0:\n",
    "            return torch.empty(0, self.embed_dim), torch.empty(0, dtype=torch.long)\n",
    "        self.encoder.eval()\n",
    "        with torch.inference_mode():\n",
    "            bases: List[torch.Tensor] = []\n",
    "            for s in self.seqs:\n",
    "                x = s.unsqueeze(0).to(self.device)\n",
    "                base = self.encoder.seq_to_basefeat(x)\n",
    "                bases.append(base.squeeze(0).cpu())\n",
    "            base_mat = torch.stack(bases, dim=0)\n",
    "            self.encoder.fit_pca(base_mat.to(self.device))\n",
    "            embs = self.encoder.transform(base_mat.to(self.device))\n",
    "        labels = torch.tensor(self.labels, dtype=torch.long)\n",
    "        return embs.detach().cpu(), labels\n",
    "\n",
    "    def update_geometry_and_conformal(self) -> None:\n",
    "        if len(self.seqs) == 0 or len(self.gestures) == 0:\n",
    "            return\n",
    "        embs_mat, labels = self._compute_embeddings_and_fit_pca()\n",
    "        if embs_mat.shape[0] == 0:\n",
    "            return\n",
    "        rng = torch.Generator().manual_seed(42)\n",
    "        idx = torch.randperm(embs_mat.shape[0], generator=rng)\n",
    "        n_total = len(idx)\n",
    "        n_calib = max(1, int(self.cfg.calib_frac * n_total))\n",
    "        calib_idx = idx[:n_calib]\n",
    "        train_idx = idx[n_calib:] if n_total - n_calib > 0 else calib_idx\n",
    "        self.gauss.fit(embs_mat[train_idx].to(self.device), labels[train_idx].to(self.device))\n",
    "        for attr in (\"mu\", \"cov\", \"chol\", \"logdet\"):\n",
    "            t = getattr(self.gauss, attr, None)\n",
    "            if t is not None:\n",
    "                setattr(self.gauss, attr, t.to(self.device))\n",
    "        self.gauss.fit_conformal(embs_mat[calib_idx].to(self.device), labels[calib_idx].to(self.device))\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def infer(self, seq_bt_d: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, Dict[str, float]]:\n",
    "        if self.num_classes == 0:\n",
    "            return torch.tensor(0.0), torch.tensor(-1), {\"reason\": \"no_classes\"}\n",
    "        z = self.encode(seq_bt_d.to(self.device))[0]\n",
    "        idx, conf, nll, margin = self.gauss.predict_with_margin(z)\n",
    "        abstain = self.gauss.abstain(idx, nll)\n",
    "        info = {\"nll\": nll, \"abstain\": bool(abstain), \"margin\": margin}\n",
    "        if abstain:\n",
    "            return torch.tensor(0.0), torch.tensor(-1), info\n",
    "        return torch.tensor(conf), torch.tensor(idx), info\n",
    "\n",
    "    def export_artifacts(self) -> Dict[str, Any]:\n",
    "        payload: Dict[str, Any] = {}\n",
    "        payload[\"cfg\"] = dict(self.cfg.__dict__)\n",
    "        payload[\"gestures\"] = [{\"name\": g.name, \"action\": g.action} for g in self.gestures]\n",
    "        payload[\"seqs\"] = [s.cpu().numpy() for s in self.seqs]\n",
    "        payload[\"labels\"] = list(map(int, self.labels))\n",
    "        if self.encoder._pca_ready:\n",
    "            payload[\"pca\"] = {\n",
    "                \"mean\": self.encoder.pca_mean.detach().cpu().numpy(),\n",
    "                \"components\": self.encoder.pca_components.detach().cpu().numpy(),\n",
    "                \"scale\": self.encoder.pca_scale.detach().cpu().numpy(),\n",
    "            }\n",
    "        if self.gauss.mu is not None:\n",
    "            payload[\"gauss\"] = {\n",
    "                \"mu\": self.gauss.mu.detach().cpu().numpy(),\n",
    "                \"cov\": self.gauss.cov.detach().cpu().numpy(),\n",
    "                \"logdet\": self.gauss.logdet.detach().cpu().numpy(),\n",
    "                \"q_global\": self.gauss.q_global,\n",
    "                \"q_per_class\": self.gauss.q_per_class,\n",
    "            }\n",
    "        return payload\n",
    "\n",
    "    def import_artifacts(self, payload: Dict[str, Any]) -> None:\n",
    "        cfg_dict = payload.get(\"cfg\", {})\n",
    "        for k, v in cfg_dict.items():\n",
    "            if hasattr(self.cfg, k):\n",
    "                setattr(self.cfg, k, v)\n",
    "        gest = payload.get(\"gestures\", [])\n",
    "        self.gestures = [Gesture(**g) for g in gest]\n",
    "        self.seqs = [torch.from_numpy(np.array(arr)).float() for arr in payload.get(\"seqs\", [])]\n",
    "        self.labels = list(map(int, payload.get(\"labels\", [])))\n",
    "        p = payload.get(\"pca\", None)\n",
    "        if p is not None:\n",
    "            self.encoder.pca_mean = torch.from_numpy(np.array(p[\"mean\"])).to(self.device).float()\n",
    "            self.encoder.pca_components = torch.from_numpy(np.array(p[\"components\"])).to(self.device).float()\n",
    "            self.encoder.pca_scale = torch.from_numpy(np.array(p[\"scale\"])).to(self.device).float()\n",
    "            self.encoder._pca_ready = True\n",
    "        else:\n",
    "            self.encoder._pca_ready = False\n",
    "        g = payload.get(\"gauss\", None)\n",
    "        if g is not None:\n",
    "            mu = torch.from_numpy(np.array(g.get(\"mu\"))).float().to(self.device)\n",
    "            cov = torch.from_numpy(np.array(g.get(\"cov\"))).float().to(self.device)\n",
    "            logdet = torch.from_numpy(np.array(g.get(\"logdet\"))).float().to(self.device)\n",
    "            K, E, _ = cov.shape\n",
    "            chol_list = []\n",
    "            for k in range(K):\n",
    "                L, info = torch.linalg.cholesky_ex(cov[k])\n",
    "                if int(info.item()) != 0:\n",
    "                    L = torch.linalg.cholesky(cov[k] + 1e-3 * torch.eye(E, device=self.device))\n",
    "                chol_list.append(L)\n",
    "            chol = torch.stack(chol_list, dim=0)\n",
    "            self.gauss.mu = mu\n",
    "            self.gauss.cov = cov\n",
    "            self.gauss.chol = chol\n",
    "            self.gauss.logdet = logdet\n",
    "            self.gauss.K = mu.shape[0]\n",
    "            self.gauss.q_global = g.get(\"q_global\", None)\n",
    "            self.gauss.q_per_class = {int(k): float(v) for k, v in g.get(\"q_per_class\", {}).items()}\n",
    "        else:\n",
    "            self.update_geometry_and_conformal()\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Unit check\n",
    "# -----------------------------\n",
    "\n",
    "def _unit_test_shapes():\n",
    "    cfg = ModelConfig()\n",
    "    mgr = ModelManager(cfg, device=torch.device(\"cpu\"))\n",
    "    T = cfg.seq_len\n",
    "    D = 42 if not cfg.use_deltas else 84\n",
    "    seq = torch.randn(T, D)\n",
    "    z = mgr.encode(seq.unsqueeze(0))\n",
    "    assert z.shape[-1] <= cfg.pca_embed_dim\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Running minimal unit check...\")\n",
    "    _unit_test_shapes()\n",
    "    print(\"OK\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
